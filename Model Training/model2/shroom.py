# -*- coding: utf-8 -*-
"""shroom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mn3yd5fEdfI1OpnfGZwr7UXTL_TOUlOs
"""

# mount google drive and download dataset from google drive

import tensorflow as tf
tf.test.gpu_device_name()

# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

from google.colab import drive
drive.mount('/content/drive')

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# 1Z8pft8C4Bu87u7Z4Vf4hNzE5TKthAlIl -- id taken from the shareable link of the zipped folder 
downloaded = drive.CreateFile({'id':"13BzrZ7w9Yt9PE7NnPhtJAFH2zDCirkbb"})   # 14 mushrooms
print('title: %s, mimeType: %s' % (downloaded['title'], downloaded['mimeType']))
downloaded.GetContentFile('image1.tgz')

from zipfile import ZipFile
file_name = "image1.tgz"

with ZipFile(file_name, 'r') as zip:
  zip.extractall("~/Content/train_data")
  print('Done')



# making the model

import numpy as np
import matplotlib.pyplot as plt
import os
import cv2 # to install this: pip install opencv-python

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras import losses
from tensorflow.keras.utils import to_categorical

from sklearn.model_selection import train_test_split

try:

    import tensorflow as tf
    import cv2
    import os
    import pickle
    import numpy as np
    print("Library Loaded Successfully ..........")
except:
    print("Library not Found ! ")


class MasterImage(object):

    def __init__(self,PATH='', IMAGE_SIZE = 50):
        self.PATH = PATH
        self.IMAGE_SIZE = IMAGE_SIZE

        self.image_data = []
        self.x_data = []
        self.y_data = []
        self.CATEGORIES = []

        # This will get List of categories
        self.list_categories = []

    def get_categories(self):
        for path in os.listdir(self.PATH):
            if '.DS_Store' in path:
                pass
            else:
                self.list_categories.append(path)
        print("Found Categories ",self.list_categories,'\n')
        return self.list_categories

    def Process_Image(self):
        try:
            """
            Return Numpy array of image
            :return: X_Data, Y_Data
            """
            self.CATEGORIES = self.get_categories()
            for categories in self.CATEGORIES:                                                  # Iterate over categories

                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path
                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification

                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder
                    new_path = os.path.join(train_folder_path, img)                             # image Path

                    try:        # if any image is corrupted
                        image_data_temp = cv2.imread(new_path,cv2.IMREAD_GRAYSCALE)                 # Read Image as numbers
                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))
                        self.image_data.append([image_temp_resize,class_index])
                    except:
                        pass

            data = np.asanyarray(self.image_data)

            # Iterate over the Data
            for x in data:
                self.x_data.append(x[0])        # Get the X_Data
                self.y_data.append(x[1])        # get the label

            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data
            Y_Data = np.asarray(self.y_data)

            # reshape x_Data

            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)

            return X_Data, Y_Data
        except:
            print("Failed to run Function Process Image ")

    def pickle_image(self):

        """
        :return: None Creates a Pickle Object of DataSet
        """
        # Call the Function and Get the Data
        X_Data,Y_Data = self.Process_Image()

        # Write the Entire Data into a Pickle File
        pickle_out = open('X_Data','wb')
        pickle.dump(X_Data, pickle_out, protocol = 4)
        pickle_out.close()

        # Write the Y Label Data
        pickle_out = open('Y_Data', 'wb')
        pickle.dump(Y_Data, pickle_out, protocol = 4)
        pickle_out.close()

        print("Pickled Image Successfully ")
        return X_Data,Y_Data

    def load_dataset(self):

        try:
            # Read the Data from Pickle Object
            X_Temp = open('X_Data','rb')
            X_Data = pickle.load(X_Temp)

            Y_Temp = open('Y_Data','rb')
            Y_Data = pickle.load(Y_Temp)

            print('Reading Dataset from PIckle Object')

            return X_Data,Y_Data

        except:
            print('Could not Found Pickle File ')
            print('Loading File and Dataset  ..........')

            X_Data,Y_Data = self.pickle_image()
            return X_Data,Y_Data

if __name__ == "__main__":
#     path = '/Users/soumilshah/IdeaProjects/mytensorflow/Dataset/training_set'
    path = '~/Content/train_data'
    a = MasterImage(PATH=path, IMAGE_SIZE=80)

    X_Data,Y_Data = a.load_dataset()
    print(X_Data.shape)

X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=0.2)

# X_Data=X_Data/255

model = Sequential()

model.add(   Conv2D(150, (3,3), input_shape = X_train.shape[1:])   )
# model.add(   Conv2D(150, (3,3), input_shape = X.shape[1:])   )
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Dropout(0.4))
model.add(Conv2D(75, (3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dense(128))

model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(64))

model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(64))

model.add(Dropout(0.4))
model.add(Dense(256))
model.add(Activation('softmax'))

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=['accuracy'])

model.fit(X_Data, Y_Data, batch_size=64, epochs=20, validation_data=(X_test, y_test))

# batch size kinda depends on the size of the dataset, 32 is being used for now for the test, might change it up to 200 or 2000 during implementation

# serializeing model to JSON

from keras.models import model_from_json

scores = model.evaluate(X_train, y_train, verbose = 0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)

# load json and create model
json_file = open('model.json','r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)



# tf save model

model.save('462shroom.model')

def prepare(filepath):
  img_size=50
  img_array=cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
  new_array=cv2.resize(img_array, (img_size, img_size))
  return new_array.reshapt(-1, img_size, img_size, 1)

model = tf.keras.models.load_model('462shroom.model')

# test image folder id: 1w7VHZFnmiu0rhNuWT5h5JV1UJAeIlmM4
# image id: 17AShz0ODQrSDdFKrxi4r6nKjG6KEsNKm

test_img = drive.CreateFile({'id':"17AShz0ODQrSDdFKrxi4r6nKjG6KEsNKm"})   # 14 mushrooms
print('title: %s, mimeType: %s' % (test_img['title'], test_img['mimeType']))
test_img.GetContentFile('test.jpg')

print(test_img)



file_name = "test.tgz"

with ZipFile(file_name, 'r') as zip:
  zip.extractall("~/Content/test_images")
  print('Done')

prediction = model.predict([prepare(test_img)])

model.save('path_to_saved_model', save_format='tf')

